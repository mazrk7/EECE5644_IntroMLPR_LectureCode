{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a2122f",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMMs)\n",
    "\n",
    "A popular choice of state space model (SSM) in time-series applications is the <b>hidden Markov model (HMM)</b>. In this notebook, we will provide an introduction on how to represent an HMM, how to perform posterior inference with this model, and how to estimate its parameters. \n",
    "\n",
    "Let's first import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696e6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget to manipulate plots in Jupyter notebooks\n",
    "%matplotlib widget \n",
    "\n",
    "import matplotlib.pyplot as plt # For general plotting\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# GPU suitable form of NumPy, also works on the CPU\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jr\n",
    "from jax import vmap\n",
    "\n",
    "# JAX library for SSMs\n",
    "from dynamax.hidden_markov_model import CategoricalHMM\n",
    "\n",
    "jnp.set_printoptions(suppress=True)\n",
    "\n",
    "# Set seed to generate reproducible \"pseudo-randomness\"\n",
    "key = jr.PRNGKey(7)\n",
    "\n",
    "plt.rc('font', size=22)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=18)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=16)    # legend fontsize\n",
    "plt.rc('figure', titlesize=22)   # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8dbdb",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "\n",
    "Markov chains are sequential models that assume an observation $\\mathbf{x}_t$ at the current timestep $t$ is a <b>sufficient statistic</b> to predict the future:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\mathbf{x}_{t+\\tau} \\, | \\, \\mathbf{x}_t, \\mathbf{x}_{1:t-1}) = p(\\mathbf{x}_{t+\\tau} \\, | \\, \\mathbf{x}_t),\n",
    "\\end{equation*}\n",
    "\n",
    "for $\\tau \\geq 0$. In other words, given the present $\\mathbf{x}_t$, the future is independent of the past. This is known as the <b>Markov assumption</b>. As a result, the joint distribution of a finite length sequence, with duration $T$, can be expressed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\mathbf{x}_{1:T}) = p(\\mathbf{x}_1) p(\\mathbf{x}_2 \\, | \\, \\mathbf{x}_1) p(\\mathbf{x}_3 \\, | \\, \\mathbf{x}_2) \\ldots = p(\\mathbf{x}_1) \\prod_{t=2}^T p(\\mathbf{x}_t \\, | \\, \\mathbf{x}_{t-1}).\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b571f19",
   "metadata": {},
   "source": [
    "## State-Space Models (SSMs)\n",
    "\n",
    "SSMs are <b>partially observed Markov chains</b>, which tells us two things about the dynamical system we are modeling:\n",
    "1. The state of this dynamical system is only \"partially observable\", meaning there is a hidden variable, $\\mathbf{z}_t \\in \\mathbb{R}^{N_z}$, that is assumed to be responsible for generating any observations, $\\mathbf{x}_t \\in \\mathbb{R}^{N_x}$, about the system's behavior.\n",
    "2. The latent state's dynamics, $\\mathbf{z}_t \\in \\mathbb{R}^{N_z}$, adhere to the Markov assumption.\n",
    "\n",
    "Please refer to the [Kalman filter (KF) notebook](kalman_filter.ipynb#Posterior-Inference-in-SSMs) for a brief on the general problem of posterior inference in SSMs. As described in the KF notebook, we often assume that the observations are conditionally independent of one another, given $\\mathbf{z}_t$, as opposed to being Markovian in their dependencies. An SSM can thus be described by the following joint distribution:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\mathbf{x}_{1:T}, \\mathbf{z}_{1:T}) = \\left[ p(\\mathbf{z}_1) \\prod_{t=2}^T p(\\mathbf{z}_t \\, | \\, \\mathbf{z}_{t-1}) \\right] \\left[ \\prod_{t=1}^T p(\\mathbf{x}_t \\, | \\, \\mathbf{z}_{t}) \\right],\n",
    "\\end{equation*}\n",
    "\n",
    "where $p(\\mathbf{z}_1)$ is the prior distribution, $p(\\mathbf{z}_t \\, | \\, \\mathbf{z}_{t-1})$ is the transition model, and $p(\\mathbf{x}_t \\, | \\, \\mathbf{z}_{t})$ is the observation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312d0a7",
   "metadata": {},
   "source": [
    "## HMM Representation\n",
    "\n",
    "HMMs are SSMs with <b>discrete</b> hidden variables, $z_t \\in \\{1, \\ldots, K\\}$. This is in contrast to linear Gaussian SSMs or linear dynamical systems, which have continuous latent variables (as described in the [KF example](kalman_filter.ipynb#Posterior-Inference-in-SSMs)). Note that the observations can be discrete, $x_t \\in \\{1, \\ldots, N_x\\}$, continuous, $\\mathbf{x}_t \\in \\mathbb{R}^{N_x}$, or a mix of both.\n",
    "\n",
    "Let us now walk through specifying each of the distributions for this generative model.\n",
    "\n",
    "### State Transition Model\n",
    "\n",
    "First, the <b>initial state distribution</b> can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(z_1 = j) = \\pi_j,\n",
    "\\end{equation*}\n",
    "\n",
    "for an arbitrary state $j$. We denote $\\boldsymbol{\\pi}$ as a categorical distribution over the $K$ states of the HMM.\n",
    "\n",
    "The <b>transition model</b> can then be denoted as:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(z_t = j | z_{t-1} = i) = A_{ij},\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{A}$ is the $K \\times K$ transition matrix (a row stochastic matrix, i.e., each row sums to one).\n",
    "\n",
    "### Observation Model\n",
    "\n",
    "Lastly, the <b>observation likelihood</b>, $p(\\mathbf{x}_t \\, | \\, z_{t} = j)$, is represented differently depending on the type of observation data, $\\mathbf{x}_t$. For discrete observations or <i>emissions</i>, $x_t \\in \\{1, \\ldots, N_x\\}$, we could utilize:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x_t = k \\, | \\, z_t = j) = B_{jk},\n",
    "\\end{equation*}\n",
    "\n",
    "with $\\mathbf{B}$ as the $K \\times N_x$ observation matrix containing the emission probabilities. A similar form could be written for Bernoulli observations. For $D$ discrete observations per time step, or $D$ <i>number of emissions</i>, a factorial model could be adopted:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\mathbf{x}_t \\, | \\, z_t = j) = \\prod_{d=1}^D \\text{Cat}(x_{td} \\, | \\, \\mathbf{B}_{d,j,:}),\n",
    "\\end{equation*}\n",
    "\n",
    "where $x_{td}$ is the $d^{th}$ observation at time $t$, and $\\mathbf{B}_{d,j,:}$ is the $j^{th}$ row of the emission probabilities matrix for observation $d$.\n",
    "\n",
    "Likewise, if $\\mathbf{x}_t \\in \\mathbb{R}^{N_x}$ is continuous, then a common choice of likelihood model is the multivariate Gaussian distribution:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\mathbf{x}_t \\, | \\, z_t = j) = \\mathcal{N}(\\mathbf{x}_t \\, | \\, \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j),\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\boldsymbol{\\mu}_j \\in \\mathbb{R}^{N_x}$ and $\\boldsymbol{\\Sigma}_j \\in \\mathbb{R}^{N_x \\times N_x}$ are the mean and covariance matrix associated with the Gaussian distribution for hidden state $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa479e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
